{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge\n",
    "- Merge on Index\n",
    "- Concatenate\n",
    "- Combining DF\n",
    "- Reshaping\n",
    "- Pivoting\n",
    "- Duplicates in DFs\n",
    "- Mapping\n",
    "- Replace\n",
    "- Rename Index\n",
    "- Binning\n",
    "- Outliers\n",
    "- Permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge - which is the JOIN - left, right, inner, outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rows as keys\n",
    "dframe1 = DataFrame({'key':['X','X','Y','Y','Z','Z'],'data_set_1':np.arange(6)})\n",
    "print(dframe1)\n",
    "\n",
    "dframe2 = DataFrame({'key':['Q','Y','Z'],'data_set_2':[100,200,300]})\n",
    "print(dframe2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge is a MANY to ONE method \n",
    "print(pd.merge(dframe1,dframe2))\n",
    "# Merge on a specific column use 'ON='\n",
    "print(pd.merge(dframe1,dframe2,on='key'))\n",
    "print(pd.merge(dframe1,dframe2,on='key',how='outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE WITH MULTIPLE KEYS\n",
    "# We can also merge with multiple keys!\n",
    "\n",
    "# Dframe on left\n",
    "df_left = DataFrame({'key1': ['Monte', 'Trevi', 'Signo'],\n",
    "                  'key2': ['Centro', 'Centro', 'Centro'],\n",
    "                  'left_data': [10,20,30]})\n",
    "\n",
    "print(df_left)\n",
    "#Dframe on right\n",
    "df_right = DataFrame({'key1': ['Monte', 'Monte', 'Trevi', 'Trevi'],\n",
    "                   'key2': ['Centro', 'Peri', 'Centro', 'Peri'],\n",
    "                   'right_data': [10,50,20,70]})\n",
    "print(df_right)\n",
    "\n",
    "#Merge\n",
    "print('Merge on Key one and Key two OUTER')\n",
    "print(pd.merge(df_left, df_right, on=['key1', 'key2'], how='outer'))\n",
    "\n",
    "# print('---')\n",
    "# print('Merge only on one Key with suffixes')\n",
    "# print(pd.merge(df_left, df_right, on=['key1'], suffixes=('_lefty','_righty')))\n",
    "\n",
    "print('---')\n",
    "print('Merge on two keys INNER')\n",
    "print(pd.merge(df_left, df_right, on=['key1', 'key2'], how='inner'))\n",
    "print('---')\n",
    "print('Merge on key1 OUTER')\n",
    "print(pd.merge(df_left, df_right, on=['key1'], how='outer'))\n",
    "print('---')\n",
    "print('Merge on key2 OUTER')\n",
    "print(pd.merge(df_left, df_right, on=['key2'], how='outer'))\n",
    "print('---')\n",
    "\n",
    "print('Merge on key1 INNER')\n",
    "print(pd.merge(df_left, df_right, on=['key1'], how='inner'))\n",
    "print('---')\n",
    "print('Merge on key2 INNER')\n",
    "\n",
    "print(pd.merge(df_left, df_right, on=['key2'], how='inner'))\n",
    "\n",
    "## Inner-Outer make sense if there are NON combo matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dframe on left\n",
    "df_left = DataFrame({'key1': ['Monte', 'Trevi', 'Signo'],\n",
    "                  'key2': ['Centro', 'Centro', 'Centro'],\n",
    "                  'left_data': [10,20,30]})\n",
    "\n",
    "print(df_left)\n",
    "#Dframe on right\n",
    "df_right = DataFrame({'key1': ['Monte', 'Monte', 'Trevi', 'Trevi'],\n",
    "                   'key2': ['Centro', 'Peri', 'Centro', 'Peri'],\n",
    "                   'right_data': [10,50,20,70]})\n",
    "\n",
    "df_right2 = df_right.copy()\n",
    "print(df_right)\n",
    "df_right2 = df_right2.set_index('key1')\n",
    "print(df_right2)\n",
    "\n",
    "print(pd.merge(df_left, df_right, on='key1', how='inner'))\n",
    "\n",
    "print(pd.merge(df_left, df_right2, on='key1', right_index=True, how='outer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example\n",
    "\n",
    "#Now let's try something a little more complicated, remember hierarchal index?\n",
    "df_left_hr = DataFrame({'key1': ['SF','SF','SF','LA','LA'],\n",
    "                   'key2': [10, 20, 30, 40, 50],\n",
    "                   'data_set': [1000, 2000, 3000, 4000, 5000]})\n",
    "df_right_hr = DataFrame(np.arange(10).reshape((5, 2)),\n",
    "                   index=[['LA','LA','SF','SF','SF'],\n",
    "                          [40, 10, 10, 10, 20]],\n",
    "                   columns=['col_1', 'col_2'])\n",
    "\n",
    "print(df_left_hr)\n",
    "print(df_right_hr)\n",
    "\n",
    "print(pd.merge(df_left_hr, df_right_hr, left_on=['key1', 'key2'], right_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate matrix on different axis\n",
    "arr1 = np.arange(9).reshape(3,3)\n",
    "\n",
    "# This is how to concatenate two matri`xes\n",
    "print(np.concatenate([arr1,arr1],axis=1))\n",
    "\n",
    "print(np.concatenate([arr1,arr1],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two series\n",
    "ser1 = Series([0,1,2], index =['A','B','C'])\n",
    "ser2 = Series([10,20], index =['A','D'])\n",
    "\n",
    "# Concatenate with markers\n",
    "print(pd.concat([ser1,ser2], keys=['ser1', 'ser2'], axis=0))\n",
    "# Concatenate the two series to obtain a dataframe\n",
    "print(pd.concat([ser1,ser2],  axis=1))\n",
    "print(pd.concat([ser1,ser2], keys=['ser1', 'ser2'],  axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two dataframes\n",
    "dframe1 = DataFrame(np.random.randn(4,3), columns=['X','Y','Z'])\n",
    "print(dframe1)\n",
    "dframe2 = DataFrame(np.random.randn(3,3), columns=['X','Y','Q'])\n",
    "print(dframe2)\n",
    "# ignore index is to get a new progressive index\n",
    "print(pd.concat([dframe1,dframe2], ignore_index=True))\n",
    "\n",
    "print(pd.concat([dframe1,dframe2], ignore_index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DF\n",
    "\n",
    "combine works on INDEXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Series\n",
    "ser1 = Series([1,2,3,np.nan,np.nan,6],\n",
    "             index=['A','B','C','D','E','F'])\n",
    "\n",
    "ser2 = Series([100,200,300,400,500,600],\n",
    "             index=['A','B','C','D','E','F'])\n",
    "\n",
    "print(Series(np.where(pd.isnull(ser1),ser2,ser1), index=ser1.index)\n",
    "     )\n",
    "print(ser1.combine_first(ser2)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Data Frames\n",
    "\n",
    "#! It fills the gaps in teh first Dataframe with the values in the second\n",
    "\n",
    "df_left_hr = DataFrame({'key1': ['SF','SF','SF','LA','LA'],\n",
    "                   'col_1': [10, 20, 30, np.nan, 50],\n",
    "                   'col_2': [1000, 2000, 3000, 4000, 5000]})\n",
    "\n",
    "\n",
    "df_right_hr = DataFrame({'key1': ['SF','LA','LA','LA','LA'],\n",
    "                   'col_1': [10, np.nan, np.nan, np.nan, 50],\n",
    "                   'col_2': [100, 200, 300, 400, 500]})\n",
    "\n",
    "\n",
    "\n",
    "print(df_left_hr)\n",
    "print(df_right_hr)\n",
    "print(df_left_hr.combine_first(df_right_hr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "\n",
    "Basically turning from long to wide format.   \n",
    "From Pandas I get a Series, if I unstuck I get back a Pandas.    \n",
    "By unstacking for diferent columns I can retunr to wide format with a different column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how stack and unstack work\n",
    "\n",
    "# Create DataFrame\n",
    "dframe1 = DataFrame(np.arange(8).reshape((2, 4)),\n",
    "                 index=pd.Index(['LA', 'SF'], name='city'),\n",
    "                 columns=pd.Index(['A', 'B', 'C','D'], name='letter'))\n",
    "#Show\n",
    "print(dframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_st = dframe1.stack()\n",
    "print(dframe_st)\n",
    "type(dframe_st)\n",
    "print(dframe_st.unstack())\n",
    "print(dframe_st.unstack('city'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe_st = dframe1.stack()\n",
    "dframe_st\n",
    "type(dframe_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some data to play with:\n",
    "\n",
    "# Note: It is not necessary to understand how this dataset was made to understand this Lecture.\n",
    "\n",
    "#import pandas testing utility\n",
    "import pandas.util.testing as tm; tm.N = 3\n",
    "\n",
    "#Create a unpivoted function\n",
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    \n",
    "    data = {'value' : frame.values.ravel('F'),\n",
    "            'variable' : np.asarray(frame.columns).repeat(N),\n",
    "            'date' : np.tile(np.asarray(frame.index), K)}\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "\n",
    "#Set the DataFrame we'll be using\n",
    "dframe = unpivot(tm.makeTimeDataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pivot the data\n",
    "\n",
    "# First two value spassed are teh row and column indexes, then finally an optional fill value\n",
    "dframe_piv = dframe.pivot('date','variable','value')\n",
    "\n",
    "#Show\n",
    "print(dframe_piv)\n",
    "\n",
    "dframe_piv.to_csv('./in_out/out_test_piv1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pivot the data\n",
    "dframe2 = dframe.copy()\n",
    "dframe2['emptycol'] = \"pivot_field\"\n",
    "print(dframe2)\n",
    "\n",
    "# First two value spassed are teh row and column indexes, then finally an optional fill value\n",
    "dframe_piv_2 = dframe2.pivot(index=['date','variable'],\n",
    "                            columns='emptycol',\n",
    "                            values='value')\n",
    "\n",
    "#Show\n",
    "print(dframe_piv_2)\n",
    "\n",
    "dframe_piv_2.to_csv('./in_out/out_test_piv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates in DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get a dataframe with duplicates\n",
    "\n",
    "dframe = DataFrame({'key1': ['A'] * 2 + ['B'] * 3,\n",
    "                  'key2': [2, 2, 2, 3, 3]})\n",
    "\n",
    "print(dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use duplicated to find duplicates\n",
    "print(dframe.duplicated())\n",
    "\n",
    "# We can also drop duplicates like this:\n",
    "print(dframe.drop_duplicates())\n",
    "\n",
    "#You can filter which duplicates to drop by a single column\n",
    "print(dframe.drop_duplicates(['key1']))\n",
    "\n",
    "#By default the first value was taken for the duplicates, we can also take the last value instead\n",
    "print(dframe.drop_duplicates(['key1'],keep='last'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping\n",
    "\n",
    "Add columns with values in a dictionary.   \n",
    "Kind of a VLOOKUP   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dframe to work with (Highest elevation cities in USA)\n",
    "dframe = DataFrame({'city':['Alma','Brian Head','Fox Park'],\n",
    "                    'altitude':[3158,3000,2762]})\n",
    "\n",
    "#Now let's say we wanted to add a column for the States, we can do that with a mapping.\n",
    "state_map={'Alma':'Colorado','Brian Head':'Utah','Fox Park':'Wyoming'}\n",
    "\n",
    "#Show\n",
    "print(dframe)\n",
    "print(state_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can map that data to our current dframe\n",
    "dframe['state'] = dframe['city'].map(state_map)\n",
    "\n",
    "print(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make  Series\n",
    "ser1 = Series([1,2,3,4,1,2,3,4])\n",
    "#Show\n",
    "print(ser1)\n",
    "\n",
    "\n",
    "# Using replace we can select --> .replace(value to be replaced, new_value)\n",
    "print(ser1.replace(1,np.nan))\n",
    "\n",
    "#Can also input lists\n",
    "print(ser1.replace([1,4],[100,400]))\n",
    "\n",
    "#Can also input dictionary\n",
    "print(ser1.replace({4:np.nan}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lec 38 - Rename Index using MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a DataFrame\n",
    "dframe= DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                 index=['NY', 'LA', 'SF'],\n",
    "                 columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "#Show\n",
    "print(dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use map to lowercase the city initials\n",
    "print(dframe.index.map(str.lower))\n",
    "\n",
    "# If you want to assign this to the actual index, you can use index\n",
    "# dframe.index = dframe.index.map(str.lower)\n",
    "\n",
    "\n",
    "# Use rename if you want to create a transformed version\n",
    "# WHITOUT modifying the original!\n",
    "\n",
    "#str.title will capitalize the first letter, lowercasing the columns\n",
    "dframe.rename(index=str.title, columns=str.lower)\n",
    "\n",
    "\n",
    "# We can also use rename with dictionaries providing new values for indexes or columns!\n",
    "# inplace=True actually edit the data\n",
    "dframe.rename(index={'ny': 'NEW YORK'},\n",
    "            columns={'A': 'ALPHA'}, inplace=True)\n",
    "\n",
    "print(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [1990,1991,1992,2008,2012,2015,1987,1969,2013,2008,1999]\n",
    "# We can seperate these years by decade\n",
    "decade_bins = [1960,1970,1980,1990,2000,2010,2020]\n",
    "\n",
    "#Now we'll use cut to get somethign called a Category object\n",
    "decade_cat = pd.cut(years,decade_bins)\n",
    "\n",
    "print(type(decade_cat))\n",
    "print(decade_cat)\n",
    "# See the categories\n",
    "print(type(decade_cat.categories))\n",
    "print(decade_cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how we would find outliers in a dataset\n",
    "\n",
    "# First we'll seed the numpy generator\n",
    "np.random.seed(12345)\n",
    "\n",
    "#Next we'll create the dataframe\n",
    "dframe = DataFrame(np.random.randn(1000,4))\n",
    "\n",
    "dframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics on the DF \n",
    "# As you can see there are values lower/greater than 3\n",
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = dframe[0]\n",
    "# Lookng at values > 3 in DF column 1 \n",
    "col[np.abs(col)>3]\n",
    "\n",
    "# Check at the entire DF - return any row with value greater than 3\n",
    "dframe[(np.abs(dframe)>3).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capping the DF\n",
    "# any value whihc is greater/smaller than 3 or -3 is replaced \n",
    "# with the value 3 times the sign +/-.\n",
    "dframe[np.abs(dframe)>3] = np.sign(dframe)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let imagine a box with 3 marbles in it: labeled 1, 2, and 3\n",
    "box = np.array([1,2,3])\n",
    "print(box)\n",
    "\n",
    "# Now lets create a random permuation WITH replacement using randint\n",
    "shaker = np.random.randint(0, len(box), size=10)\n",
    "print(shaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY - RECAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge\n",
    "- Merge on Index\n",
    "- Concatenate\n",
    "- Combining DF\n",
    "- Reshaping\n",
    "- Pivoting\n",
    "- Duplicates in DFs\n",
    "- Mapping\n",
    "- Replace\n",
    "- Rename Index\n",
    "- Binning\n",
    "- Outliers\n",
    "- Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
